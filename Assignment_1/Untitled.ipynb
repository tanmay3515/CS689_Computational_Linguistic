{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db5f8142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "file_path = \"hi_100_1.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    corpus = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3bdd5d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_vowels = ['अ', 'आ', 'इ', 'ई', 'उ', 'ऊ', 'ए', 'ऐ', 'ओ', 'औ', 'अं', 'अः']\n",
    "hindi_matras = ['ा', 'ि', 'ी', 'ु', 'ू', 'े', 'ै', 'ो', 'ौ', 'ं', 'ः', '्', 'ृ']\n",
    "hindi_punctuation_symbols = \"#,।,?!.:;‘’“”-…()▁ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789\"\n",
    "\n",
    "# This function is used to extract words\n",
    "def extract_words(text):\n",
    "  pattern = r'[\\u0900-\\u097F]+'\n",
    "  words = re.findall(pattern, text)\n",
    "  return words\n",
    "\n",
    "# Unicode correction function\n",
    "def correction(s):\n",
    "\n",
    "  new = []\n",
    "  halant = '\\u094D'\n",
    "  for i in range(len(s)):\n",
    "\n",
    "    if(s[i] == '.' or s[i] == ',' or s[i] == '?' or s[i] == '!' or s[i] == ':' or s[i] == ';' or s[i] == '\"' or s[i] == \"'\" or s[i] == '–' or s[i] == '_' or s[i] == '(' or s[i] == ')' or s[i] == '[' or s[i] == ']' or s[i] == '/' or s[i] == '|' or s[i]== ' '):\n",
    "      continue\n",
    "    elif(s[i] in hindi_punctuation_symbols):\n",
    "      continue\n",
    "    elif(s[i] in hindi_vowels):\n",
    "      new.append(s[i])\n",
    "    elif(s[i]=='ा'):\n",
    "        new.append('आ')\n",
    "    elif(s[i]=='ि'):\n",
    "        new.append('इ')\n",
    "    elif(s[i]=='ी'):\n",
    "        new.append('ई')\n",
    "    elif(s[i]=='ु'):\n",
    "        new.append('उ')\n",
    "    elif(s[i]=='ू'):\n",
    "        new.append('ऊ')\n",
    "    elif(s[i]=='े'):\n",
    "        new.append('ए')\n",
    "    elif(s[i]=='ै'):\n",
    "        new.append('ऐ')\n",
    "    elif(s[i]=='ो'):\n",
    "        new.append('ओ')\n",
    "    elif(s[i]=='ौ'):\n",
    "        new.append('औ')\n",
    "    elif(s[i]=='ं'):\n",
    "        new.append('अं')\n",
    "    elif(s[i]=='ः'):\n",
    "        new.append('अः')\n",
    "    elif(s[i]=='ृ'):\n",
    "        new.append('र' + halant)\n",
    "    elif(s[i] == '्'):\n",
    "      continue\n",
    "    elif(i+1<len(s) and s[i+1] in hindi_matras):\n",
    "      new.append(s[i] + halant)\n",
    "    else:\n",
    "      new.append(s[i] + halant)\n",
    "      new.append('अ')\n",
    "  return new\n",
    "\n",
    "# Making function to store character in decreasing order of frequency\n",
    "def frequency_decreasing(array, array_):\n",
    "  element_freq = Counter(array_)\n",
    "  sorted_elements = sorted(element_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "  element_map = {element: freq for element, freq in sorted_elements}\n",
    "  return element_map\n",
    "\n",
    "#This function makes bigram\n",
    "def makebigram(array):\n",
    "  bigram = []\n",
    "  for i in range(len(array)):\n",
    "    for j in range(len(array[i])-1):\n",
    "      bigram.append(array[i][j] + array[i][j+1])\n",
    "  return bigram\n",
    "\n",
    "\n",
    "# This function make syllables\n",
    "def make_syllable(k):\n",
    "  new = []\n",
    "  j=0\n",
    "  while(j<len(k)):\n",
    "    if(k[j] in hindi_vowels):\n",
    "      while(j<len(k) and k[j] in hindi_vowels):\n",
    "        new.append(k[j])\n",
    "        j = j+1\n",
    "    else:\n",
    "      m = ''\n",
    "      while(j<len(k) and k[j] not in hindi_vowels):\n",
    "        m = m + k[j]\n",
    "        j=j+1\n",
    "      if(j>=len(k)):\n",
    "        new.append(m[:-1])\n",
    "      else:\n",
    "        new.append(m[:-1] + character_map[k[j]])\n",
    "      j=j+1\n",
    "  return new\n",
    "\n",
    "# This function calculates precision, recall and f1 score\n",
    "def prf(ground_truth, tokens):\n",
    "\n",
    "  precisions = []\n",
    "  recalls = []\n",
    "  f1_scores = []\n",
    "\n",
    "  for ground_truth_sublist, model_sublist in zip(ground_truth, tokens):\n",
    "      ground_truth_set = set(ground_truth_sublist)\n",
    "      model_set = set(model_sublist)\n",
    "      true_positives = len(ground_truth_set.intersection(model_set))\n",
    "      false_positives = len(model_set - ground_truth_set)\n",
    "      false_negatives = len(ground_truth_set - model_set)\n",
    "\n",
    "      precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "      recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "      f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "      \n",
    "      precisions.append(precision)\n",
    "      recalls.append(recall)\n",
    "      f1_scores.append(f1_score)\n",
    "        \n",
    "  average_precision = sum(precisions) / len(precisions)\n",
    "  average_recall = sum(recalls) / len(recalls)\n",
    "  average_f1_score = sum(f1_scores) / len(f1_scores)\n",
    "      \n",
    "  return (average_precision, average_recall, average_f1_score)\n",
    "character_map = {\n",
    "    'अ': '',\n",
    "    'आ': 'ा',\n",
    "    'इ': 'ि',\n",
    "    'ई': 'ी',\n",
    "    'उ': 'ु',\n",
    "    'ऊ': 'ू',\n",
    "    'ए': 'े',\n",
    "    'ऐ': 'ै',\n",
    "    'ओ': 'ो',\n",
    "    'औ': 'ौ',\n",
    "    'अं': 'ं',\n",
    "    'अः': 'ः'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8030dd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 1\n",
    "\n",
    "words = extract_words(corpus)\n",
    "corrected_unicode=[]\n",
    "\n",
    "# Storing the corrected unicode in array of list\n",
    "for i in range(len(words)):\n",
    "  corrected_unicode.append(correction(words[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eff7273b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Frequent Unigram Characters:\n",
      "अ: 7287721\n",
      "आ: 2991109\n",
      "ए: 2318442\n",
      "क्: 2219964\n",
      "र्: 2140164\n",
      "ई: 1460305\n",
      "इ: 1432973\n",
      "न्: 1334448\n",
      "स्: 1283708\n",
      "अं: 1201207\n",
      "ह्: 1133159\n",
      "म्: 1053237\n",
      "त्: 980066\n",
      "ल्: 919917\n",
      "ओ: 896588\n",
      "प्: 805896\n",
      "य्: 752819\n",
      "व्: 624743\n",
      "द्: 607633\n",
      "उ: 587149\n",
      "Top 20 Frequent Bigram Characters:\n",
      "र्अ: 1173271\n",
      "अर्: 792442\n",
      "क्अ: 619365\n",
      "स्अ: 518851\n",
      "न्अ: 515849\n",
      "अन्: 437698\n",
      "क्ए: 407130\n",
      "प्अ: 405662\n",
      "अह्: 390412\n",
      "आर्: 368512\n",
      "एअं: 359377\n",
      "अक्: 355687\n",
      "त्अ: 353275\n",
      "ल्अ: 333314\n",
      "न्ए: 328953\n",
      "म्अ: 324938\n",
      "क्आ: 314321\n",
      "अत्: 308837\n",
      "य्आ: 297778\n",
      "ह्ऐ: 297199\n"
     ]
    }
   ],
   "source": [
    "#Question 2 : Unigram and Bigram of Characters\n",
    "\n",
    "# character_array stores the converted list from array of list (array).\n",
    "character_array = [char for sublist in corrected_unicode for char in sublist]\n",
    "character_bigram = makebigram(corrected_unicode)\n",
    "# array_map stores the decreasing order of unigram characters\n",
    "corrected_unicode_map = frequency_decreasing(corrected_unicode, character_array)\n",
    "# bigram_map stores the decreasing order of bigram characters\n",
    "bigram_map = frequency_decreasing(character_bigram, character_bigram)\n",
    "\n",
    "c = 20\n",
    "\n",
    "# Printing Top 20 unigrams and bigrams characters\n",
    "print(\"Top 20 Frequent Unigram Characters:\")\n",
    "for element, freq in corrected_unicode_map.items():\n",
    "  if(c==0):\n",
    "    break\n",
    "  c = c-1\n",
    "  print(f\"{element}: {freq}\")\n",
    "c=20\n",
    "print(\"Top 20 Frequent Bigram Characters:\")\n",
    "for element, freq in bigram_map.items():\n",
    "  if(c==0):\n",
    "    break\n",
    "  c = c-1\n",
    "  print(f\"{element}: {freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96a43b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3 : Making Syllable\n",
    "\n",
    "character_map = {\n",
    "    'अ': '',\n",
    "    'आ': 'ा',\n",
    "    'इ': 'ि',\n",
    "    'ई': 'ी',\n",
    "    'उ': 'ु',\n",
    "    'ऊ': 'ू',\n",
    "    'ए': 'े',\n",
    "    'ऐ': 'ै',\n",
    "    'ओ': 'ो',\n",
    "    'औ': 'ौ',\n",
    "    'अं': 'ं',\n",
    "    'अः': 'ः'\n",
    "}\n",
    "\n",
    "\n",
    "syllable = []\n",
    "\n",
    "# Storing Syllables in array of list (syllable)\n",
    "for i in range(len(corrected_unicode)):\n",
    "  syllable.append(make_syllable(corrected_unicode[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f7fb75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Frequent Unigram Syllables:\n",
      "र: 1008411\n",
      "अं: 988234\n",
      "क: 609542\n",
      "न: 507657\n",
      "स: 494360\n",
      "के: 405005\n",
      "प: 392089\n",
      "ल: 330146\n",
      "ने: 328102\n",
      "का: 307954\n",
      "त: 306238\n",
      "है: 297019\n",
      "म: 295118\n",
      "मे: 292706\n",
      "ए: 288662\n",
      "ह: 278771\n",
      "अ: 253201\n",
      "ब: 247540\n",
      "की: 235320\n",
      "ग: 226337\n",
      "Top 20 Frequent Bigram Syllables:\n",
      "मेअं: 259224\n",
      "कर: 160043\n",
      "और: 115671\n",
      "पर: 99665\n",
      "इस: 82886\n",
      "हैअं: 80715\n",
      "हीअं: 55632\n",
      "एक: 54576\n",
      "लिए: 54024\n",
      "नही: 49367\n",
      "अप: 45157\n",
      "कार: 39111\n",
      "किया: 37430\n",
      "योअं: 34861\n",
      "रने: 34496\n",
      "कहा: 33135\n",
      "यह: 31419\n",
      "गया: 30278\n",
      "सर: 30119\n",
      "उन: 29754\n"
     ]
    }
   ],
   "source": [
    "# Unigram and Bigram of Syllable\n",
    "\n",
    "# character_array stores the converted list from array of list (array).\n",
    "syllable_array = [char for sublist in syllable for char in sublist]\n",
    "syllable_bigram = makebigram(syllable)\n",
    "syllable_map = frequency_decreasing(syllable, syllable_array)\n",
    "syllable_bigram_map = frequency_decreasing(syllable_bigram, syllable_bigram)\n",
    "\n",
    "c = 20\n",
    "# Printing Top 20 Unigram and Bigram Syllables\n",
    "print(\"Top 20 Frequent Unigram Syllables:\")\n",
    "for element, freq in syllable_map.items():\n",
    "  if(c==0):\n",
    "    break;\n",
    "  c=c-1\n",
    "  print(f\"{element}: {freq}\")\n",
    "\n",
    "c=20\n",
    "print(\"Top 20 Frequent Bigram Syllables:\")\n",
    "for element, freq in syllable_bigram_map.items():\n",
    "  if(c==0):\n",
    "    break;\n",
    "  c=c-1\n",
    "\n",
    "  print(f\"{element}: {freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88d06f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (0.1.99)\n",
      "Requirement already satisfied: torch in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (2.2.0)\n",
      "Requirement already satisfied: torchvision in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (0.17.0)\n",
      "Requirement already satisfied: torchaudio in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (2.2.0)\n",
      "Requirement already satisfied: filelock in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (from torch) (2023.4.0)\n",
      "Requirement already satisfied: numpy in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (from torchvision) (1.24.3)\n",
      "Requirement already satisfied: requests in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (from requests->torchvision) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (from requests->torchvision) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: tensorflow in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (2.15.0)\n",
      "Requirement already satisfied: tensorflow-macos==2.15.0 in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.24.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: setuptools in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (4.9.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (0.36.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (1.60.1)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.15.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (from tensorflow-macos==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow-macos==2.15.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.28.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (from tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/tanmaydubey/anaconda3/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-macos==2.15.0->tensorflow) (3.2.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-gpu\n",
      "  Using cached tensorflow-gpu-2.12.0.tar.gz (2.6 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[44 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/tanmaydubey/anaconda3/lib/python3.11/site-packages/setuptools/_vendor/packaging/requirements.py\", line 35, in __init__\n",
      "  \u001b[31m   \u001b[0m     parsed = _parse_requirement(requirement_string)\n",
      "  \u001b[31m   \u001b[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/tanmaydubey/anaconda3/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py\", line 64, in parse_requirement\n",
      "  \u001b[31m   \u001b[0m     return _parse_requirement(Tokenizer(source, rules=DEFAULT_RULES))\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/tanmaydubey/anaconda3/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py\", line 82, in _parse_requirement\n",
      "  \u001b[31m   \u001b[0m     url, specifier, marker = _parse_requirement_details(tokenizer)\n",
      "  \u001b[31m   \u001b[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/tanmaydubey/anaconda3/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py\", line 126, in _parse_requirement_details\n",
      "  \u001b[31m   \u001b[0m     marker = _parse_requirement_marker(\n",
      "  \u001b[31m   \u001b[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/tanmaydubey/anaconda3/lib/python3.11/site-packages/setuptools/_vendor/packaging/_parser.py\", line 147, in _parse_requirement_marker\n",
      "  \u001b[31m   \u001b[0m     tokenizer.raise_syntax_error(\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/tanmaydubey/anaconda3/lib/python3.11/site-packages/setuptools/_vendor/packaging/_tokenizer.py\", line 165, in raise_syntax_error\n",
      "  \u001b[31m   \u001b[0m     raise ParserSyntaxError(\n",
      "  \u001b[31m   \u001b[0m setuptools.extern.packaging._tokenizer.ParserSyntaxError: Expected end or semicolon (after name and no valid version specifier)\n",
      "  \u001b[31m   \u001b[0m     python_version>\"3.7\"\n",
      "  \u001b[31m   \u001b[0m                   ^\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m The above exception was the direct cause of the following exception:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/mr/f14mxc55085gps_ctlj58f940000gn/T/pip-install-o1308sre/tensorflow-gpu_e94265e5e03245f396c800e94a9de1c6/setup.py\", line 40, in <module>\n",
      "  \u001b[31m   \u001b[0m     setuptools.setup()\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/tanmaydubey/anaconda3/lib/python3.11/site-packages/setuptools/__init__.py\", line 106, in setup\n",
      "  \u001b[31m   \u001b[0m     _install_setup_requires(attrs)\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/tanmaydubey/anaconda3/lib/python3.11/site-packages/setuptools/__init__.py\", line 77, in _install_setup_requires\n",
      "  \u001b[31m   \u001b[0m     dist.parse_config_files(ignore_option_errors=True)\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/tanmaydubey/anaconda3/lib/python3.11/site-packages/setuptools/dist.py\", line 900, in parse_config_files\n",
      "  \u001b[31m   \u001b[0m     self._finalize_requires()\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/tanmaydubey/anaconda3/lib/python3.11/site-packages/setuptools/dist.py\", line 597, in _finalize_requires\n",
      "  \u001b[31m   \u001b[0m     self._move_install_requirements_markers()\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/tanmaydubey/anaconda3/lib/python3.11/site-packages/setuptools/dist.py\", line 637, in _move_install_requirements_markers\n",
      "  \u001b[31m   \u001b[0m     inst_reqs = list(_reqs.parse(spec_inst_reqs))\n",
      "  \u001b[31m   \u001b[0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/tanmaydubey/anaconda3/lib/python3.11/site-packages/setuptools/_vendor/packaging/requirements.py\", line 37, in __init__\n",
      "  \u001b[31m   \u001b[0m     raise InvalidRequirement(str(e)) from e\n",
      "  \u001b[31m   \u001b[0m setuptools.extern.packaging.requirements.InvalidRequirement: Expected end or semicolon (after name and no valid version specifier)\n",
      "  \u001b[31m   \u001b[0m     python_version>\"3.7\"\n",
      "  \u001b[31m   \u001b[0m                   ^\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip3 install sentencepiece\n",
    "!pip3 install torch torchvision torchaudio\n",
    "!pip install tensorflow\n",
    "!pip install tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27fc9ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: hi_100_1.txt\n",
      "  input_format: \n",
      "  model_prefix: unigram(1).model\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: hi_100_1.txt\n",
      "trainer_interface.cc(378) LOG(WARNING) Found too long line (5994 > 4192).\n",
      "trainer_interface.cc(380) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(381) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 298383 sentences\n",
      "trainer_interface.cc(414) LOG(INFO) Skipped 456 too long sentences.\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=39649689\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9502% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=153\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999502\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 298381 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=18991333\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 395429 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 298381\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 324920\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 324920 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=156332 obj=10.8621 num_tokens=702942 num_tokens/piece=4.49647\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=134610 obj=8.92531 num_tokens=703641 num_tokens/piece=5.22726\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=100903 obj=8.88768 num_tokens=727710 num_tokens/piece=7.21198\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=100760 obj=8.87633 num_tokens=727853 num_tokens/piece=7.22363\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=75563 obj=8.91628 num_tokens=768262 num_tokens/piece=10.1672\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=75557 obj=8.90731 num_tokens=768256 num_tokens/piece=10.1679\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=56667 obj=8.96362 num_tokens=813830 num_tokens/piece=14.3616\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=56667 obj=8.95147 num_tokens=813800 num_tokens/piece=14.3611\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=42500 obj=9.0328 num_tokens=863216 num_tokens/piece=20.311\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=42500 obj=9.0174 num_tokens=863193 num_tokens/piece=20.3104\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=31875 obj=9.12149 num_tokens=915167 num_tokens/piece=28.7111\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=31875 obj=9.10142 num_tokens=915173 num_tokens/piece=28.7113\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=23906 obj=9.23421 num_tokens=969065 num_tokens/piece=40.5365\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=23906 obj=9.21287 num_tokens=969074 num_tokens/piece=40.5369\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=17929 obj=9.37773 num_tokens=1024155 num_tokens/piece=57.1228\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=17929 obj=9.34571 num_tokens=1024169 num_tokens/piece=57.1236\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=13446 obj=9.55358 num_tokens=1079886 num_tokens/piece=80.3128\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=13446 obj=9.51426 num_tokens=1079931 num_tokens/piece=80.3162\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=10084 obj=9.75948 num_tokens=1136993 num_tokens/piece=112.752\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=10084 obj=9.7122 num_tokens=1137021 num_tokens/piece=112.755\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=8800 obj=9.83652 num_tokens=1162246 num_tokens/piece=132.073\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=8800 obj=9.81312 num_tokens=1162321 num_tokens/piece=132.082\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: unigram(1).model.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: unigram(1).model.vocab\n"
     ]
    }
   ],
   "source": [
    "# Question 4\n",
    "\n",
    "import sentencepiece as spm\n",
    "from transformers import BertTokenizer\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "corpus_file = \"hi_100_1.txt\"\n",
    "\n",
    "#Unigram\n",
    "spm_model_file = \"unigram(1).model\"\n",
    "spm.SentencePieceTrainer.train(input=corpus_file, model_prefix=spm_model_file, model_type='unigram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a8e89fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: hi_100_1.txt\n",
      "  input_format: \n",
      "  model_prefix: bpe(1).model\n",
      "  model_type: BPE\n",
      "  vocab_size: 1000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: hi_100_1.txt\n",
      "trainer_interface.cc(378) LOG(WARNING) Found too long line (5994 > 4192).\n",
      "trainer_interface.cc(380) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(381) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 298383 sentences\n",
      "trainer_interface.cc(414) LOG(INFO) Skipped 456 too long sentences.\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=39649689\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9502% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=153\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999502\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 298381 sentences.\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 298381\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 324920\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1364729 min_freq=1990\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=220725 size=20 all=7449 active=2256 piece=▁ल\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=117261 size=40 all=8844 active=3651 piece=िक\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=74160 size=60 all=10391 active=5198 piece=▁थ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=58450 size=80 all=12553 active=7360 piece=त्र\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=43528 size=100 all=14682 active=9489 piece=▁सं\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=43241 min_freq=4149\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=35408 size=120 all=16900 active=3171 piece=▁किया\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=28955 size=140 all=18442 active=4713 piece=न्ह\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=24411 size=160 all=20405 active=6676 piece=▁राज\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=21882 size=180 all=22297 active=8568 piece=▁सर\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=19406 size=200 all=24144 active=10415 piece=▁चु\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=19305 min_freq=2919\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=17837 size=220 all=26375 active=3387 piece=गर\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16215 size=240 all=27984 active=4996 piece=▁जी\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14802 size=260 all=29833 active=6845 piece=रो\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13436 size=280 all=31773 active=8785 piece=▁उप\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12360 size=300 all=32981 active=9993 piece=▁पुलिस\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=12356 min_freq=1941\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11890 size=320 all=34340 active=2990 piece=▁इसके\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10951 size=340 all=35982 active=4632 piece=चार\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10320 size=360 all=37937 active=6587 piece=▁द्\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9811 size=380 all=39130 active=7780 piece=▁परि\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9445 size=400 all=41055 active=9705 piece=्यों\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=9415 min_freq=1429\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8960 size=420 all=42288 active=3222 piece=▁आय\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8386 size=440 all=43529 active=4463 piece=▁दौर\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8077 size=460 all=45064 active=5998 piece=▁चल\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7730 size=480 all=46422 active=7356 piece=▁सकता\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7442 size=500 all=47949 active=8883 piece=▁चुनाव\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=7424 min_freq=1125\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7087 size=520 all=49692 active=4134 piece=▁भारतीय\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6786 size=540 all=51326 active=5768 piece=▁अव\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6635 size=560 all=52202 active=6644 piece=पर\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6283 size=580 all=53798 active=8240 piece=▁क्र\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5999 size=600 all=55396 active=9838 piece=भाव\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=5982 min_freq=904\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5774 size=620 all=56792 active=4071 piece=▁बो\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5563 size=640 all=58473 active=5752 piece=योग\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5284 size=660 all=59469 active=6748 piece=▁इससे\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5123 size=680 all=60536 active=7815 piece=▁कुमार\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4968 size=700 all=62054 active=9333 piece=ुक\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4952 min_freq=761\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4780 size=720 all=63542 active=4436 piece=वल\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4563 size=740 all=64700 active=5594 piece=▁इस्\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4433 size=760 all=65843 active=6737 piece=▁आम\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4313 size=780 all=66568 active=7462 piece=▁आने\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4158 size=800 all=67465 active=8359 piece=▁बंद\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4156 min_freq=659\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4025 size=820 all=68540 active=4417 piece=डे\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3923 size=840 all=69446 active=5323 piece=▁इसमें\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: bpe(1).model.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: bpe(1).model.vocab\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: hi_100_1.txt\n",
      "  input_format: \n",
      "  model_prefix: bpe(2).model\n",
      "  model_type: BPE\n",
      "  vocab_size: 2000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: hi_100_1.txt\n",
      "trainer_interface.cc(378) LOG(WARNING) Found too long line (5994 > 4192).\n",
      "trainer_interface.cc(380) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(381) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 298383 sentences\n",
      "trainer_interface.cc(414) LOG(INFO) Skipped 456 too long sentences.\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=39649689\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9502% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=153\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999502\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 298381 sentences.\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 298381\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 324920\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1364729 min_freq=1990\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=220725 size=20 all=7449 active=2256 piece=▁ल\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=117261 size=40 all=8844 active=3651 piece=िक\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=74160 size=60 all=10391 active=5198 piece=▁थ\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=58450 size=80 all=12553 active=7360 piece=त्र\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=43528 size=100 all=14682 active=9489 piece=▁सं\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=43241 min_freq=4149\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=35408 size=120 all=16900 active=3171 piece=▁किया\n",
      "bpe_model_trainer.cc(268) LOG(I"
     ]
    }
   ],
   "source": [
    "#bpe\n",
    "\n",
    "spm_model_file1 = \"bpe(1).model\"\n",
    "spm.SentencePieceTrainer.train(input=corpus_file, model_prefix=spm_model_file1, model_type='bpe', vocab_size=1000)\n",
    "spm_model_file2 = \"bpe(2).model\"\n",
    "spm.SentencePieceTrainer.train(input=corpus_file, model_prefix=spm_model_file2, model_type='bpe', vocab_size=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4495ed81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mbert\n",
    "\n",
    "tokenizer_mbert1 = BertTokenizer.from_pretrained('bert-base-multilingual-cased' , max_length=1000, truncation=True)\n",
    "tokenizer_mbert2 = BertTokenizer.from_pretrained('bert-base-multilingual-cased' , max_length=2000, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5aca56d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unigram\n",
    "\n",
    "#loading the pretrained model\n",
    "spm_tokenizer = spm.SentencePieceProcessor()\n",
    "spm_tokenizer.load(spm_model_file + \".model\")\n",
    "\n",
    "unigram_tokens = []\n",
    "with open(corpus_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        line_tokens = spm_tokenizer.encode_as_pieces(line.strip())\n",
    "        unigram_tokens.append(line_tokens)\n",
    "\n",
    "unigram_tokens = [[word.replace('▁', '') for word in sublist] for sublist in unigram_tokens]\n",
    "# print(\"Tokens:\", unigram_tokens, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7371b721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 frequent unigram tokens\n",
      "के: 328232\n",
      "।: 276509\n",
      "में: 240924\n",
      "है: 215430\n",
      "की: 200481\n",
      ",: 187662\n",
      "को: 155131\n",
      "से: 147523\n",
      "ने: 127318\n",
      "का: 126932\n",
      ".: 126589\n",
      "और: 115832\n",
      "पर: 97696\n",
      "कि: 83341\n",
      "हैं: 80190\n",
      "ी: 68624\n",
      "भी: 65963\n",
      "कर: 64756\n",
      "न: 62018\n",
      "ों: 58423\n",
      "Top 20 frequent unigram characters\n",
      "अ: 9533555\n",
      "के्: 328232\n",
      "में्: 240924\n",
      "है्: 215430\n",
      "की्: 200481\n",
      "को्: 155131\n",
      "से्: 147523\n",
      "ने्: 127318\n",
      "का्: 126932\n",
      "और्: 115832\n",
      "पर्: 97696\n",
      "कि्: 83341\n",
      "ई: 81703\n",
      "हैं्: 80190\n",
      "भी्: 65963\n",
      "कर्: 64756\n",
      "न्: 62018\n",
      "आ: 60828\n",
      "ों्: 58423\n",
      "एक्: 54601\n",
      "Top 20 frequent unigram bigram characters\n",
      "के्अ: 328144\n",
      "अके्: 319878\n",
      "में्अ: 240848\n",
      "अमें्: 236115\n",
      "है्अ: 215244\n",
      "अहै्: 212427\n",
      "की्अ: 200345\n",
      "अकी्: 195729\n",
      "को्अ: 155082\n",
      "अको्: 150607\n",
      "से्अ: 147438\n",
      "असे्: 143507\n",
      "ने्अ: 127127\n",
      "का्अ: 126080\n",
      "अने्: 122790\n",
      "अका्: 122166\n",
      "और्अ: 115817\n",
      "अऔर्: 111071\n",
      "पर्अ: 96338\n",
      "अपर्: 95236\n"
     ]
    }
   ],
   "source": [
    "unigram_tokens_array = [char for sublist in unigram_tokens for char in sublist]\n",
    "unigram_tokens_map = frequency_decreasing(unigram_tokens, unigram_tokens_array)\n",
    "\n",
    "array_unigram =[]\n",
    "\n",
    "# Storing the corrected unicode in array of list\n",
    "for i in range(len(unigram_tokens)):\n",
    "  array_unigram.append(correction(unigram_tokens[i]))\n",
    "\n",
    "unigram_array = [char for sublist in array_unigram for char in sublist]\n",
    "unigram_bigram = makebigram(array_unigram)\n",
    "unigram_array_map = frequency_decreasing(array_unigram, unigram_array)\n",
    "unigram_bigram_map = frequency_decreasing(unigram_bigram, unigram_bigram)\n",
    "\n",
    "count=0\n",
    "print(\"Top 20 frequent unigram tokens\")\n",
    "for element, freq in unigram_tokens_map.items():\n",
    "    print(f\"{element}: {freq}\")\n",
    "    count += 1\n",
    "    if count == 20:\n",
    "        break\n",
    "print(\"Top 20 frequent unigram characters\")\n",
    "count=0\n",
    "for element, freq in unigram_array_map.items():\n",
    "    print(f\"{element}: {freq}\")\n",
    "    count += 1\n",
    "    if count == 20:\n",
    "        break\n",
    "print(\"Top 20 frequent unigram bigram characters\")\n",
    "count=0\n",
    "for element, freq in unigram_bigram_map.items():\n",
    "    print(f\"{element}: {freq}\")\n",
    "    count += 1\n",
    "    if count == 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06461fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 frequent unigram syllable\n",
      "के: 328101\n",
      "में: 240827\n",
      "है: 215240\n",
      "की: 200325\n",
      "को: 155018\n",
      "से: 147428\n",
      "ने: 127048\n",
      "का: 126065\n",
      "और: 115810\n",
      "पर: 96326\n",
      "कि: 83187\n",
      "हैं: 80248\n",
      "भी: 65950\n",
      "कर: 63361\n",
      "न: 60719\n",
      "ों: 58380\n",
      "एक: 53760\n",
      "इस: 50400\n",
      "लिए: 49269\n",
      "नहीं: 47367\n",
      "Top 20 frequent unigram syllable bigram\n",
      "केलिए: 43527\n",
      "हैकि: 25213\n",
      "केसाथ: 18386\n",
      "कहाकि: 16143\n",
      "केबाद: 14510\n",
      "रहाहै: 12017\n",
      "नेकहा: 11913\n",
      "हैऔर: 11139\n",
      "गयाहै: 11052\n",
      "ोंके: 10859\n",
      "रहेहैं: 10178\n",
      "ोंमें: 8908\n",
      "करनेके: 8850\n",
      "रहीहै: 8753\n",
      "ोंको: 8419\n",
      "जाताहै: 7335\n",
      "ोंकी: 7247\n",
      "कियागया: 6892\n",
      "सकताहै: 6516\n",
      "नहींहै: 6508\n"
     ]
    }
   ],
   "source": [
    "unigram_syllable = []\n",
    "for i in range(len(array_unigram)):\n",
    "  unigram_syllable.append(make_syllable(array_unigram[i]))\n",
    "\n",
    "unigram_syllable_array = [char for sublist in unigram_syllable for char in sublist]\n",
    "unigram_syllable_bigram = makebigram(unigram_syllable)\n",
    "unigram_syllable_array_map = frequency_decreasing(unigram_syllable, unigram_syllable_array)\n",
    "unigram_syllable_bigram_map = frequency_decreasing(unigram_syllable_bigram, unigram_syllable_bigram)\n",
    "\n",
    "print(\"Top 20 frequent unigram syllable\")\n",
    "count =0\n",
    "for element, freq in unigram_syllable_array_map.items():\n",
    "    print(f\"{element}: {freq}\")\n",
    "    count += 1\n",
    "    if count == 20:\n",
    "        break\n",
    "print(\"Top 20 frequent unigram syllable bigram\")\n",
    "count =0\n",
    "for element, freq in unigram_syllable_bigram_map.items():\n",
    "    print(f\"{element}: {freq}\")\n",
    "    count += 1\n",
    "    if count == 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "069a95ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text from the corpus using a simple whitespace tokenizer\n",
    "vocab = set()\n",
    "with open(corpus_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        tokens = line.strip().split()\n",
    "        vocab.update(tokens)\n",
    "\n",
    "# Step 2: Use the pretrained tokenizer to tokenize the corpus\n",
    "# Define a function to tokenize text using the pretrained whitespace tokenizer\n",
    "def tokenize_with_whitespace(text):\n",
    "    return text.split()\n",
    "\n",
    "# Tokenize the corpus using the pretrained whitespace tokenizer\n",
    "whitespace_tokens = []\n",
    "with open(corpus_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        line_tokens = tokenize_with_whitespace(line.strip())\n",
    "        whitespace_tokens.append(line_tokens)\n",
    "\n",
    "# print(\"Corpus tokens:\", whitespace_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb5b5f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 frequent whitespace tokens\n",
      "के: 316314\n",
      "में: 237426\n",
      "की: 189769\n",
      "को: 145266\n",
      "से: 136979\n",
      "और: 114563\n",
      "का: 109743\n",
      "ने: 102378\n",
      "पर: 88560\n",
      "है।: 88260\n",
      "कि: 76741\n",
      "है: 72656\n",
      "भी: 63920\n",
      "एक: 49279\n",
      "लिए: 47955\n",
      "इस: 47558\n",
      "कर: 44563\n",
      "नहीं: 44168\n",
      "ही: 40104\n",
      "तो: 33469\n",
      "Top 20 frequent whitespace characters\n",
      "अ: 7979923\n",
      "के्: 316314\n",
      "में्: 237426\n",
      "की्: 189769\n",
      "को्: 145266\n",
      "से्: 136979\n",
      "और्: 114563\n",
      "का्: 109743\n",
      "ने्: 102378\n",
      "पर्: 88560\n",
      "है।्: 88260\n",
      "कि्: 76741\n",
      "है्: 72656\n",
      "भी्: 63920\n",
      "एक्: 49279\n",
      "लिए्: 47955\n",
      "इस्: 47558\n",
      "कर्: 44563\n",
      "नहीं्: 44168\n",
      "ही्: 40104\n",
      "Top 20 frequent whitespace bigram characters\n",
      "के्अ: 316314\n",
      "अके्: 316168\n",
      "में्अ: 237426\n",
      "अमें्: 237351\n",
      "की्अ: 189769\n",
      "अकी्: 189711\n",
      "को्अ: 145265\n",
      "अको्: 145224\n",
      "से्अ: 136978\n",
      "असे्: 136926\n",
      "और्अ: 114562\n",
      "अऔर्: 113826\n",
      "का्अ: 109743\n",
      "अका्: 109702\n",
      "ने्अ: 102378\n",
      "अने्: 102357\n",
      "पर्अ: 88560\n",
      "अपर्: 88270\n",
      "अहै।्: 88260\n",
      "है।्अ: 88259\n"
     ]
    }
   ],
   "source": [
    "whitespace_tokens_array = [char for sublist in whitespace_tokens for char in sublist]\n",
    "whitespace_tokens_map = frequency_decreasing(whitespace_tokens, whitespace_tokens_array)\n",
    "\n",
    "array_whitespace =[]\n",
    "\n",
    "# Storing the corrected unicode in array of list\n",
    "for i in range(len(whitespace_tokens)):\n",
    "  array_whitespace.append(correction(whitespace_tokens[i]))\n",
    "\n",
    "whitespace_array = [char for sublist in array_whitespace for char in sublist]\n",
    "whitespace_bigram = makebigram(array_whitespace)\n",
    "whitespace_array_map = frequency_decreasing(array_whitespace, whitespace_array)\n",
    "whitespace_bigram_map = frequency_decreasing(whitespace_bigram,whitespace_bigram)\n",
    "\n",
    "print(\"Top 20 frequent whitespace tokens\")\n",
    "count=0\n",
    "for element, freq in whitespace_tokens_map.items():\n",
    "    print(f\"{element}: {freq}\")\n",
    "    count += 1\n",
    "    if count == 20:\n",
    "        break\n",
    "print(\"Top 20 frequent whitespace characters\")\n",
    "count=0\n",
    "for element, freq in whitespace_array_map.items():\n",
    "    print(f\"{element}: {freq}\")\n",
    "    count += 1\n",
    "    if count == 20:\n",
    "        break\n",
    "\n",
    "print(\"Top 20 frequent whitespace bigram characters\")\n",
    "count=0\n",
    "for element, freq in whitespace_bigram_map.items():\n",
    "    print(f\"{element}: {freq}\")\n",
    "    count += 1\n",
    "    if count == 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93cb128d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 frequent whitespace syllable\n",
      "के: 316315\n",
      "में: 237429\n",
      "की: 189781\n",
      "को: 145285\n",
      "से: 136979\n",
      "और: 114562\n",
      "का: 109752\n",
      "ने: 102378\n",
      "पर: 88560\n",
      "है।: 88259\n",
      "कि: 76742\n",
      "है: 72651\n",
      "भी: 63920\n",
      "एक: 49279\n",
      "लिए: 47955\n",
      "इस: 47558\n",
      "कर: 44563\n",
      "नहीं: 44168\n",
      "ही: 40104\n",
      "तो: 33467\n",
      "Top 20 frequent whitespace syllable bigram\n",
      "केलिए: 42449\n",
      "हैकि: 24818\n",
      "केसाथ: 17075\n",
      "कहाकि: 15922\n",
      "केबाद: 14163\n",
      "हैऔर: 10176\n",
      "नेकहा: 9098\n",
      "करनेके: 8826\n",
      "बतायाकि: 6393\n",
      "कोलेकर: 5977\n",
      "गयाहै।: 5678\n",
      "रहाहै।: 5483\n",
      "केखिलाफ: 5312\n",
      "केदौरान: 5162\n",
      "केबीच: 5117\n",
      "बारेमें: 5059\n",
      "करतेहुए: 4817\n",
      "रहेहैं।: 4701\n",
      "मेंभी: 4682\n",
      "कररहे: 4637\n"
     ]
    }
   ],
   "source": [
    "whitespace_syllable = []\n",
    "for i in range(len(array_whitespace)):\n",
    "  whitespace_syllable.append(make_syllable(array_whitespace[i]))\n",
    "\n",
    "whitespace_syllable_array = [char for sublist in whitespace_syllable for char in sublist]\n",
    "whitespace_syllable_bigram = makebigram(whitespace_syllable)\n",
    "whitespace_syllable_array_map = frequency_decreasing(whitespace_syllable, whitespace_syllable_array)\n",
    "whitespace_syllable_bigram_map = frequency_decreasing(whitespace_syllable_bigram, whitespace_syllable_bigram)\n",
    "\n",
    "print(\"Top 20 frequent whitespace syllable\")\n",
    "count =0\n",
    "for element, freq in whitespace_syllable_array_map.items():\n",
    "    print(f\"{element}: {freq}\")\n",
    "    count += 1\n",
    "    if count == 20:\n",
    "        break\n",
    "\n",
    "print(\"Top 20 frequent whitespace syllable bigram\")\n",
    "count =0\n",
    "for element, freq in whitespace_syllable_bigram_map.items():\n",
    "    print(f\"{element}: {freq}\")\n",
    "    count += 1\n",
    "    if count == 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8c9213cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bpe(1k)\n",
    "\n",
    "# Load the pretrained SentencePiece model\n",
    "spm_tokenizer1 = spm.SentencePieceProcessor()\n",
    "spm_tokenizer1.load(spm_model_file2 + \".model\")\n",
    "\n",
    "# Tokenize text using the pretrained SentencePiece model\n",
    "bpe1_tokens = []\n",
    "with open(corpus_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        line_tokens = spm_tokenizer1.encode_as_pieces(line.strip())\n",
    "        bpe1_tokens.append(line_tokens)\n",
    "\n",
    "bpe1_tokens = [[word.replace('▁', '') for word in sublist] for sublist in bpe1_tokens]\n",
    "\n",
    "# print(\"Tokens:\", bpe1_tokens, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c574dd3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 frequent bpe1 tokens\n",
      "के: 344230\n",
      "।: 276966\n",
      "में: 245494\n",
      "है: 215519\n",
      "की: 214647\n",
      ",: 189721\n",
      "ने: 178508\n",
      "को: 164642\n",
      "से: 161960\n",
      "का: 147974\n",
      "क: 133331\n",
      ".: 128642\n",
      "न: 124503\n",
      "और: 115365\n",
      "म: 114131\n",
      "स: 111941\n",
      "पर: 105525\n",
      "प: 99336\n",
      "व: 96389\n",
      "ल: 95954\n",
      "Top 20 frequent bpe1 characters\n",
      "अ: 12231325\n",
      "के्: 344230\n",
      "में्: 245494\n",
      "है्: 215519\n",
      "की्: 214647\n",
      "ने्: 178508\n",
      "को्: 164642\n",
      "से्: 161960\n",
      "का्: 147974\n",
      "क्: 133331\n",
      "न्: 124503\n",
      "और्: 115365\n",
      "म्: 114131\n",
      "स्: 111941\n",
      "पर्: 105525\n",
      "आ: 103570\n",
      "प्: 99336\n",
      "व्: 96389\n",
      "ल्: 95954\n",
      "र्: 94915\n",
      "Top 20 frequent bpe1 bigram characters\n",
      "के्अ: 344183\n",
      "अके्: 331989\n",
      "में्अ: 245417\n",
      "अमें्: 240224\n",
      "है्अ: 215418\n",
      "की्अ: 214455\n",
      "अहै्: 211637\n",
      "अकी्: 206486\n",
      "ने्अ: 178474\n",
      "अने्: 170637\n",
      "को्अ: 164620\n",
      "से्अ: 161938\n",
      "अको्: 157836\n",
      "असे्: 155233\n",
      "का्अ: 146247\n",
      "अका्: 141103\n",
      "क्अ: 128022\n",
      "अक्: 119440\n",
      "न्अ: 117931\n",
      "और्अ: 115333\n"
     ]
    }
   ],
   "source": [
    "bpe1_tokens_array = [char for sublist in bpe1_tokens for char in sublist]\n",
    "bpe1_tokens_map = frequency_decreasing(bpe1_tokens, bpe1_tokens_array)\n",
    "\n",
    "array_bpe1 =[]\n",
    "\n",
    "# Storing the corrected unicode in array of list\n",
    "for i in range(len(bpe1_tokens)):\n",
    "  array_bpe1.append(correction(bpe1_tokens[i]))\n",
    "# print(array_bpe1)\n",
    "\n",
    "bpe1_array = [char for sublist in array_bpe1 for char in sublist]\n",
    "bpe1_bigram = makebigram(array_bpe1)\n",
    "bpe1_array_map = frequency_decreasing(array_bpe1, bpe1_array)\n",
    "bpe1_bigram_map = frequency_decreasing(bpe1_bigram, bpe1_bigram)\n",
    "\n",
    "print(\"Top 20 frequent bpe1 tokens\")\n",
    "count=0\n",
    "for element, freq in bpe1_tokens_map.items():\n",
    "    print(f\"{element}: {freq}\")\n",
    "    count += 1\n",
    "    if count == 20:\n",
    "        break\n",
    "\n",
    "print(\"Top 20 frequent bpe1 characters\")\n",
    "count =0\n",
    "for element, freq in bpe1_array_map.items():\n",
    "    print(f\"{element}: {freq}\")\n",
    "    count += 1\n",
    "    if count == 20:\n",
    "        break\n",
    "\n",
    "print(\"Top 20 frequent bpe1 bigram characters\")\n",
    "count =0\n",
    "for element, freq in bpe1_bigram_map.items():\n",
    "    print(f\"{element}: {freq}\")\n",
    "    count += 1\n",
    "    if count == 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "91767d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 frequent bpe1 syllable\n",
      "के: 343997\n",
      "में: 245409\n",
      "है: 216118\n",
      "की: 214227\n",
      "ने: 178414\n",
      "को: 164568\n",
      "से: 161904\n",
      "का: 146105\n",
      "क: 126772\n",
      "न: 117230\n",
      "और: 115327\n",
      "म: 108427\n",
      "स: 106006\n",
      "पर: 104157\n",
      "कि: 92752\n",
      "व: 92328\n",
      "प: 91697\n",
      "ल: 90946\n",
      "त: 89628\n",
      "कर: 87811\n",
      "Top 20 frequent bpe1 syllable bigram\n",
      "के: 343997\n",
      "में: 245409\n",
      "है: 216118\n",
      "की: 214227\n",
      "ने: 178414\n",
      "को: 164568\n",
      "से: 161904\n",
      "का: 146105\n",
      "क: 126772\n",
      "न: 117230\n",
      "और: 115327\n",
      "म: 108427\n",
      "स: 106006\n",
      "पर: 104157\n",
      "कि: 92752\n",
      "व: 92328\n",
      "प: 91697\n",
      "ल: 90946\n",
      "त: 89628\n",
      "कर: 87811\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bpe1_syllable = []\n",
    "\n",
    "# Storing Syllables in array of list (syllable)\n",
    "for i in range(len(array_bpe1)):\n",
    "  bpe1_syllable.append(make_syllable(array_bpe1[i]))\n",
    "\n",
    "bpe1_syllable_array = [char for sublist in bpe1_syllable for char in sublist]\n",
    "bpe1_syllable_bigram = makebigram(bpe1_syllable)\n",
    "bpe1_syllable_array_map = frequency_decreasing(bpe1_syllable, bpe1_syllable_array)\n",
    "bpe1_syllable_bigram_map = frequency_decreasing(bpe1_syllable_bigram, bpe1_syllable_bigram)\n",
    "\n",
    "print(\"Top 20 frequent bpe1 syllable\")\n",
    "count =0\n",
    "for element, freq in bpe1_syllable_array_map.items():\n",
    "    print(f\"{element}: {freq}\")\n",
    "    count += 1\n",
    "    if count == 20:\n",
    "        break\n",
    "        \n",
    "print(\"Top 20 frequent bpe1 syllable bigram\")\n",
    "count =0\n",
    "for element, freq in bpe1_syllable_array_map.items():\n",
    "    print(f\"{element}: {freq}\")\n",
    "    count += 1\n",
    "    if count == 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f89b7c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bpe(2k)\n",
    "\n",
    "# Load the pretrained SentencePiece model\n",
    "spm_tokenizer2 = spm.SentencePieceProcessor()\n",
    "spm_tokenizer2.load(spm_model_file + \".model\")\n",
    "\n",
    "# Tokenize text using the pretrained SentencePiece model\n",
    "bpe2_tokens = []\n",
    "with open(corpus_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        line_tokens = spm_tokenizer2.encode_as_pieces(line.strip())\n",
    "        bpe2_tokens.append(line_tokens)\n",
    "\n",
    "bpe2_tokens = [[word.replace('▁', '') for word in sublist] for sublist in bpe2_tokens]\n",
    "\n",
    "# print(\"Tokens:\", bpe2_tokens, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1eb39bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 frequent bpe2 tokens\n",
      "के: 328232\n",
      "।: 276509\n",
      "में: 240924\n",
      "है: 215430\n",
      "की: 200481\n",
      ",: 187662\n",
      "को: 155131\n",
      "से: 147523\n",
      "ने: 127318\n",
      "का: 126932\n",
      ".: 126589\n",
      "और: 115832\n",
      "पर: 97696\n",
      "कि: 83341\n",
      "हैं: 80190\n",
      "ी: 68624\n",
      "भी: 65963\n",
      "कर: 64756\n",
      "न: 62018\n",
      "ों: 58423\n",
      "Top 20 frequent bpe2 characters\n",
      "अ: 9533555\n",
      "के्: 328232\n",
      "में्: 240924\n",
      "है्: 215430\n",
      "की्: 200481\n",
      "को्: 155131\n",
      "से्: 147523\n",
      "ने्: 127318\n",
      "का्: 126932\n",
      "और्: 115832\n",
      "पर्: 97696\n",
      "कि्: 83341\n",
      "ई: 81703\n",
      "हैं्: 80190\n",
      "भी्: 65963\n",
      "कर्: 64756\n",
      "न्: 62018\n",
      "आ: 60828\n",
      "ों्: 58423\n",
      "एक्: 54601\n",
      "Top 20 frequent bpe2 bigram characters\n",
      "के्अ: 328144\n",
      "अके्: 319878\n",
      "में्अ: 240848\n",
      "अमें्: 236115\n",
      "है्अ: 215244\n",
      "अहै्: 212427\n",
      "की्अ: 200345\n",
      "अकी्: 195729\n",
      "को्अ: 155082\n",
      "अको्: 150607\n",
      "से्अ: 147438\n",
      "असे्: 143507\n",
      "ने्अ: 127127\n",
      "का्अ: 126080\n",
      "अने्: 122790\n",
      "अका्: 122166\n",
      "और्अ: 115817\n",
      "अऔर्: 111071\n",
      "पर्अ: 96338\n",
      "अपर्: 95236\n"
     ]
    }
   ],
   "source": [
    "bpe2_tokens_array = [char for sublist in bpe2_tokens for char in sublist]\n",
    "bpe2_tokens_map = frequency_decreasing(bpe2_tokens, bpe2_tokens_array)\n",
    "\n",
    "array_bpe2 =[]\n",
    "\n",
    "# Storing the corrected unicode in array of list\n",
    "for i in range(len(bpe2_tokens)):\n",
    "  array_bpe2.append(correction(bpe2_tokens[i]))\n",
    "\n",
    "bpe2_array = [char for sublist in array_bpe2 for char in sublist]\n",
    "bpe2_bigram = makebigram(array_bpe2)\n",
    "bpe2_array_map = frequency_decreasing(array_bpe2, bpe2_array)\n",
    "bpe2_bigram_map = frequency_decreasing(bpe2_bigram, bpe2_bigram)\n",
    "\n",
    "print(\"Top 20 frequent bpe2 tokens\")\n",
    "count=0\n",
    "for element, freq in bpe2_tokens_map.items():\n",
    "    print(f\"{element}: {freq}\")\n",
    "    count += 1\n",
    "    if count == 20:\n",
    "        break\n",
    "        \n",
    "print(\"Top 20 frequent bpe2 characters\")\n",
    "count =0\n",
    "for element, freq in bpe2_array_map.items():\n",
    "    print(f\"{element}: {freq}\")\n",
    "    count += 1\n",
    "    if count == 20:\n",
    "        break\n",
    "        \n",
    "print(\"Top 20 frequent bpe2 bigram characters\")\n",
    "count =0\n",
    "for element, freq in bpe2_bigram_map.items():\n",
    "    print(f\"{element}: {freq}\")\n",
    "    count += 1\n",
    "    if count == 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a37192f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 frequent bpe2 syllable\n",
      "के: 328101\n",
      "में: 240827\n",
      "है: 215240\n",
      "की: 200325\n",
      "को: 155018\n",
      "से: 147428\n",
      "ने: 127048\n",
      "का: 126065\n",
      "और: 115810\n",
      "पर: 96326\n",
      "कि: 83187\n",
      "हैं: 80248\n",
      "भी: 65950\n",
      "कर: 63361\n",
      "न: 60719\n",
      "ों: 58380\n",
      "एक: 53760\n",
      "इस: 50400\n",
      "लिए: 49269\n",
      "नहीं: 47367\n",
      "Top 20 frequent bpe2 syllable bigram\n",
      "केलिए: 43527\n",
      "हैकि: 25213\n",
      "केसाथ: 18386\n",
      "कहाकि: 16143\n",
      "केबाद: 14510\n",
      "रहाहै: 12017\n",
      "नेकहा: 11913\n",
      "हैऔर: 11139\n",
      "गयाहै: 11052\n",
      "ोंके: 10859\n",
      "रहेहैं: 10178\n",
      "ोंमें: 8908\n",
      "करनेके: 8850\n",
      "रहीहै: 8753\n",
      "ोंको: 8419\n",
      "जाताहै: 7335\n",
      "ोंकी: 7247\n",
      "कियागया: 6892\n",
      "सकताहै: 6516\n",
      "नहींहै: 6508\n"
     ]
    }
   ],
   "source": [
    "bpe2_syllable = []\n",
    "\n",
    "# Storing Syllables in array of list (syllable)\n",
    "for i in range(len(array_bpe2)):\n",
    "  bpe2_syllable.append(make_syllable(array_bpe2[i]))\n",
    "\n",
    "bpe2_syllable_array = [char for sublist in bpe2_syllable for char in sublist]\n",
    "bpe2_syllable_bigram = makebigram(bpe2_syllable)\n",
    "bpe2_syllable_array_map = frequency_decreasing(bpe2_syllable, bpe2_syllable_array)\n",
    "bpe2_syllable_bigram_map = frequency_decreasing(bpe2_syllable_bigram, bpe2_syllable_bigram)\n",
    "\n",
    "print(\"Top 20 frequent bpe2 syllable\")\n",
    "count =0\n",
    "for element, freq in bpe2_syllable_array_map.items():\n",
    "    print(f\"{element}: {freq}\")\n",
    "    count += 1\n",
    "    if count == 20:\n",
    "        break\n",
    "        \n",
    "print(\"Top 20 frequent bpe2 syllable bigram\")\n",
    "count =0\n",
    "for element, freq in bpe2_syllable_bigram_map.items():\n",
    "    print(f\"{element}: {freq}\")\n",
    "    count += 1\n",
    "    if count == 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8f0f066",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mbert(1k)\n",
    "\n",
    "# Tokenize text from the corpus file\n",
    "mbert1_tokens = []\n",
    "with open(corpus_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        line_tokens = tokenizer_mbert1.tokenize(line.strip())\n",
    "        mbert1_tokens.append(line_tokens)\n",
    "\n",
    "mbert1_tokens = [[word.replace('#', '') for word in sublist] for sublist in mbert1_tokens]\n",
    "\n",
    "# print(\"Tokens:\", mbert1_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4e065c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 frequent mbert1 tokens\n",
      "के: 347164\n",
      "स: 330691\n",
      "प: 314836\n",
      "म: 305742\n",
      "र: 301686\n",
      "।: 276966\n",
      "ब: 259624\n",
      "में: 246133\n",
      "न: 235575\n",
      "है: 215601\n",
      "की: 213189\n",
      "ा: 205239\n",
      "क: 193113\n",
      ",: 191460\n",
      "ल: 187736\n",
      "को: 173966\n",
      "ी: 173352\n",
      "ने: 172673\n",
      "से: 165303\n",
      "का: 164733\n",
      "Top 20 frequent mbert1 characters\n",
      "अ: 13074272\n",
      "के्: 347164\n",
      "स्: 330691\n",
      "आ: 322009\n",
      "प्: 314836\n",
      "र्: 310412\n",
      "म्: 305742\n",
      "ब्: 259624\n",
      "में्: 246133\n",
      "न्: 235575\n",
      "है्: 215601\n",
      "की्: 213189\n",
      "ए: 212540\n",
      "ई: 211699\n",
      "क्: 193113\n",
      "ल्: 187736\n",
      "को्: 173966\n",
      "ने्: 172673\n",
      "उ: 169745\n",
      "से्: 165303\n",
      "Top 20 frequent mbert1 bigram characters\n",
      "के्अ: 346133\n",
      "अके्: 312718\n",
      "अर्: 283213\n",
      "अस्: 282231\n",
      "प्अ: 275911\n",
      "र्अ: 270918\n",
      "स्अ: 262927\n",
      "अम्: 259483\n",
      "म्अ: 247881\n",
      "में्अ: 246053\n",
      "अप्: 232125\n",
      "अमें्: 230108\n",
      "ब्अ: 218393\n",
      "अन्: 217907\n",
      "है्अ: 215492\n",
      "की्अ: 213003\n",
      "अब्: 211205\n",
      "न्अ: 202768\n",
      "अहै्: 197757\n",
      "अकी्: 193750\n"
     ]
    }
   ],
   "source": [
    "mbert1_tokens_array = [char for sublist in mbert1_tokens for char in sublist]\n",
    "mbert1_tokens_map = frequency_decreasing(mbert1_tokens, mbert1_tokens_array)\n",
    "\n",
    "array_mbert1 =[]\n",
    "\n",
    "# Storing the corrected unicode in array of list\n",
    "for i in range(len(mbert1_tokens)):\n",
    "  array_mbert1.append(correction(mbert1_tokens[i]))\n",
    "\n",
    "mbert1_array = [char for sublist in array_mbert1 for char in sublist]\n",
    "mbert1_bigram = makebigram(array_mbert1)\n",
    "mbert1_array_map = frequency_decreasing(array_mbert1, mbert1_array)\n",
    "mbert1_bigram_map = frequency_decreasing(mbert1_bigram, mbert1_bigram)\n",
    "\n",
    "print(\"Top 20 frequent mbert1 tokens\")\n",
    "count=0\n",
    "for element, freq in mbert1_tokens_map.items():\n",
    "    print(f\"{element}: {freq}\")\n",
    "    count += 1\n",
    "    if count == 20:\n",
    "        break\n",
    "        \n",
    "print(\"Top 20 frequent mbert1 characters\")\n",
    "count =0\n",
    "for element, freq in mbert1_array_map.items():\n",
    "    print(f\"{element}: {freq}\")\n",
    "    count += 1\n",
    "    if count == 20:\n",
    "        break\n",
    "        \n",
    "print(\"Top 20 frequent mbert1 bigram characters\")\n",
    "count =0\n",
    "for element, freq in mbert1_bigram_map.items():\n",
    "    print(f\"{element}: {freq}\")\n",
    "    count += 1\n",
    "    if count == 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebe8cc02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 frequent mbert1 syllable\n",
      "के: 346078\n",
      "र: 270891\n",
      "प: 264116\n",
      "स: 262879\n",
      "म: 247864\n",
      "में: 246032\n",
      "ब: 216914\n",
      "है: 216299\n",
      "की: 212996\n",
      "न: 202748\n",
      "को: 173909\n",
      "ने: 171802\n",
      "ल: 168702\n",
      "क: 168195\n",
      "से: 164752\n",
      "का: 163816\n",
      "ज: 138768\n",
      "अ: 133579\n",
      "त: 117607\n",
      "आ: 117004\n",
      "Top 20 frequent mbert1 syllable bigram\n",
      "केलिए: 44086\n",
      "प्र: 36824\n",
      "आप: 26545\n",
      "हैकि: 25541\n",
      "केसाथ: 18502\n",
      "ोंके: 17268\n",
      "कहाकि: 16140\n",
      "बता: 15908\n",
      "ोंमें: 15470\n",
      "सम: 15308\n",
      "केबाद: 14486\n",
      "ोंको: 13862\n",
      "पुल: 13812\n",
      "जर: 13166\n",
      "ुलिस: 12626\n",
      "नेकहा: 12036\n",
      "रहाहै: 12017\n",
      "माम: 11749\n",
      "जाए: 11590\n",
      "ोंकी: 11249\n"
     ]
    }
   ],
   "source": [
    "mbert1_syllable = []\n",
    "\n",
    "# Storing Syllables in array of list (syllable)\n",
    "for i in range(len(array_mbert1)):\n",
    "  mbert1_syllable.append(make_syllable(array_mbert1[i]))\n",
    "\n",
    "mbert1_syllable_array = [char for sublist in mbert1_syllable for char in sublist]\n",
    "mbert1_syllable_bigram = makebigram(mbert1_syllable)\n",
    "mbert1_syllable_array_map = frequency_decreasing(mbert1_syllable, mbert1_syllable_array)\n",
    "mbert1_syllable_bigram_map = frequency_decreasing(mbert1_syllable_bigram, mbert1_syllable_bigram)\n",
    "\n",
    "print(\"Top 20 frequent mbert1 syllable\")\n",
    "count =0\n",
    "for element, freq in mbert1_syllable_array_map.items():\n",
    "    print(f\"{element}: {freq}\")\n",
    "    count += 1\n",
    "    if count == 20:\n",
    "        break\n",
    "        \n",
    "print(\"Top 20 frequent mbert1 syllable bigram\")\n",
    "count =0\n",
    "for element, freq in mbert1_syllable_bigram_map.items():\n",
    "    print(f\"{element}: {freq}\")\n",
    "    count += 1\n",
    "    if count == 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "987e981a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mbert(2k)\n",
    "\n",
    "# Tokenize text from the corpus file\n",
    "mbert2_tokens = []\n",
    "with open(corpus_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        line_tokens = tokenizer_mbert2.tokenize(line.strip())\n",
    "        mbert2_tokens.append(line_tokens)\n",
    "\n",
    "mbert2_tokens = [[word.replace('#', '') for word in sublist] for sublist in mbert2_tokens]\n",
    "\n",
    "# print(\"Tokens:\", mbert2_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9457c516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 frequent mbert2 tokens\n",
      "के: 347164\n",
      "स: 330691\n",
      "प: 314836\n",
      "म: 305742\n",
      "र: 301686\n",
      "।: 276966\n",
      "ब: 259624\n",
      "में: 246133\n",
      "न: 235575\n",
      "है: 215601\n",
      "की: 213189\n",
      "ा: 205239\n",
      "क: 193113\n",
      ",: 191460\n",
      "ल: 187736\n",
      "को: 173966\n",
      "ी: 173352\n",
      "ने: 172673\n",
      "से: 165303\n",
      "का: 164733\n",
      "Top 20 frequent mbert2 characters\n",
      "अ: 13074272\n",
      "के्: 347164\n",
      "स्: 330691\n",
      "आ: 322009\n",
      "प्: 314836\n",
      "र्: 310412\n",
      "म्: 305742\n",
      "ब्: 259624\n",
      "में्: 246133\n",
      "न्: 235575\n",
      "है्: 215601\n",
      "की्: 213189\n",
      "ए: 212540\n",
      "ई: 211699\n",
      "क्: 193113\n",
      "ल्: 187736\n",
      "को्: 173966\n",
      "ने्: 172673\n",
      "उ: 169745\n",
      "से्: 165303\n",
      "Top 20 frequent mbert2 bigram characters\n",
      "के्अ: 346133\n",
      "अके्: 312718\n",
      "अर्: 283213\n",
      "अस्: 282231\n",
      "प्अ: 275911\n",
      "र्अ: 270918\n",
      "स्अ: 262927\n",
      "अम्: 259483\n",
      "म्अ: 247881\n",
      "में्अ: 246053\n",
      "अप्: 232125\n",
      "अमें्: 230108\n",
      "ब्अ: 218393\n",
      "अन्: 217907\n",
      "है्अ: 215492\n",
      "की्अ: 213003\n",
      "अब्: 211205\n",
      "न्अ: 202768\n",
      "अहै्: 197757\n",
      "अकी्: 193750\n"
     ]
    }
   ],
   "source": [
    "mbert2_tokens_array = [char for sublist in mbert2_tokens for char in sublist]\n",
    "mbert2_tokens_map = frequency_decreasing(mbert2_tokens, mbert2_tokens_array)\n",
    "\n",
    "\n",
    "array_mbert2 =[]\n",
    "\n",
    "# Storing the corrected unicode in array of list\n",
    "for i in range(len(mbert2_tokens)):\n",
    "  array_mbert2.append(correction(mbert2_tokens[i]))\n",
    "\n",
    "mbert2_array = [char for sublist in array_mbert2 for char in sublist]\n",
    "mbert2_bigram = makebigram(array_mbert2)\n",
    "mbert2_array_map = frequency_decreasing(array_mbert2, mbert2_array)\n",
    "mbert2_bigram_map = frequency_decreasing(mbert2_bigram, mbert2_bigram)\n",
    "\n",
    "print(\"Top 20 frequent mbert2 tokens\")\n",
    "count=0\n",
    "for element, freq in mbert2_tokens_map.items():\n",
    "    print(f\"{element}: {freq}\")\n",
    "    count += 1\n",
    "    if count == 20:\n",
    "        break\n",
    "        \n",
    "print(\"Top 20 frequent mbert2 characters\")\n",
    "count =0\n",
    "for element, freq in mbert2_array_map.items():\n",
    "    print(f\"{element}: {freq}\")\n",
    "    count += 1\n",
    "    if count == 20:\n",
    "        break\n",
    "        \n",
    "print(\"Top 20 frequent mbert2 bigram characters\")\n",
    "count =0\n",
    "for element, freq in mbert2_bigram_map.items():\n",
    "    print(f\"{element}: {freq}\")\n",
    "    count += 1\n",
    "    if count == 20:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "551f1602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 frequent mbert2 syllable\n",
      "के: 346078\n",
      "र: 270891\n",
      "प: 264116\n",
      "स: 262879\n",
      "म: 247864\n",
      "में: 246032\n",
      "ब: 216914\n",
      "है: 216299\n",
      "की: 212996\n",
      "न: 202748\n",
      "को: 173909\n",
      "ने: 171802\n",
      "ल: 168702\n",
      "क: 168195\n",
      "से: 164752\n",
      "का: 163816\n",
      "ज: 138768\n",
      "अ: 133579\n",
      "त: 117607\n",
      "आ: 117004\n",
      "Top 20 frequent mbert2 syllable bigram\n",
      "केलिए: 44086\n",
      "प्र: 36824\n",
      "आप: 26545\n",
      "हैकि: 25541\n",
      "केसाथ: 18502\n",
      "ोंके: 17268\n",
      "कहाकि: 16140\n",
      "बता: 15908\n",
      "ोंमें: 15470\n",
      "सम: 15308\n",
      "केबाद: 14486\n",
      "ोंको: 13862\n",
      "पुल: 13812\n",
      "जर: 13166\n",
      "ुलिस: 12626\n",
      "नेकहा: 12036\n",
      "रहाहै: 12017\n",
      "माम: 11749\n",
      "जाए: 11590\n",
      "ोंकी: 11249\n"
     ]
    }
   ],
   "source": [
    "mbert2_syllable = []\n",
    "\n",
    "# Storing Syllables in array of list (syllable)\n",
    "for i in range(len(array_mbert2)):\n",
    "  mbert2_syllable.append(make_syllable(array_mbert2[i]))\n",
    "\n",
    "mbert2_syllable_array = [char for sublist in mbert2_syllable for char in sublist]\n",
    "mbert2_syllable_bigram = makebigram(mbert2_syllable)\n",
    "mbert2_syllable_array_map = frequency_decreasing(mbert2_syllable, mbert2_syllable_array)\n",
    "mbert2_syllable_bigram_map = frequency_decreasing(mbert2_syllable_bigram, mbert2_syllable_bigram)\n",
    "\n",
    "print(\"Top 20 frequent mbert2 syllable\")\n",
    "count =0\n",
    "for element, freq in mbert2_syllable_array_map.items():\n",
    "    print(f\"{element}: {freq}\")\n",
    "    count += 1\n",
    "    if count == 20:\n",
    "        break\n",
    "        \n",
    "print(\"Top 20 frequent mbert2 syllable bigram\")\n",
    "count =0\n",
    "for element, freq in mbert2_syllable_bigram_map.items():\n",
    "    print(f\"{element}: {freq}\")\n",
    "    count += 1\n",
    "    if count == 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "efb69862",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tanmaydubey/anaconda3/lib/python3.11/site-packages/transformers/utils/generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "#indicbert\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "indic_model = AutoModel.from_pretrained('ai4bharat/indic-bert')\n",
    "tokenizer_indicbert1 = AutoTokenizer.from_pretrained('ai4bharat/indic-bert',max_length=1000, truncation=True)\n",
    "tokenizer_indicbert2 = AutoTokenizer.from_pretrained('ai4bharat/indic-bert', max_length=2000, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "727868c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#indic_bert(1k)\n",
    "\n",
    "indicbert1_tokens = []\n",
    "with open(corpus_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        line_tokens1 = tokenizer_indicbert1.tokenize(line.strip())\n",
    "        indicbert1_tokens.append(line_tokens1)\n",
    "\n",
    "indicbert1_tokens = [[word.replace('_', '') for word in sublist] for sublist in indicbert1_tokens]\n",
    "\n",
    "# for i in range(500):\n",
    "#print(\"Tokenized Output:\", indicbert1_tokens[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16af0f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 frequent indicbert1 tokens\n",
      "▁क: 960351\n",
      "▁ह: 444966\n",
      "य: 385246\n",
      "▁म: 283842\n",
      "।: 267276\n",
      "त: 263047\n",
      "▁: 262384\n",
      "▁स: 242014\n",
      "▁पर: 241561\n",
      "▁न: 225179\n",
      "न: 190411\n",
      "क: 188131\n",
      ",: 180710\n",
      "स: 146070\n",
      "ए: 134291\n",
      "▁कर: 125257\n",
      "▁द: 116469\n",
      "▁और: 115095\n",
      "र: 114722\n",
      "ग: 109763\n",
      "Top 20 frequent indicbert1 characters\n",
      "अ: 12432590\n",
      "▁क्: 960351\n",
      "▁ह्: 444966\n",
      "य्: 385246\n",
      "▁म्: 283842\n",
      "त्: 263047\n",
      "▁स्: 242014\n",
      "▁पर्: 241561\n",
      "▁न्: 225179\n",
      "न्: 190411\n",
      "क्: 188131\n",
      "स्: 146070\n",
      "ए: 134291\n",
      "▁कर्: 125257\n",
      "▁द्: 116469\n",
      "▁और्: 115095\n",
      "र्: 114722\n",
      "ग्: 109763\n",
      "ष्: 101499\n",
      "ल्: 101277\n",
      "Top 20 frequent indicbert1 bigram characters\n",
      "▁क्अ: 960351\n",
      "अ▁क्: 930416\n",
      "▁ह्अ: 444966\n",
      "अ▁ह्: 430998\n",
      "य्अ: 385246\n",
      "अय्: 384935\n",
      "▁म्अ: 283842\n",
      "अ▁म्: 275704\n",
      "त्अ: 263047\n",
      "अत्: 262594\n",
      "▁स्अ: 242014\n",
      "▁पर्अ: 241561\n",
      "अ▁स्: 233746\n",
      "अ▁पर्: 230754\n",
      "▁न्अ: 225179\n",
      "अ▁न्: 217531\n",
      "न्अ: 190411\n",
      "अन्: 190394\n",
      "क्अ: 188131\n",
      "अक्: 187850\n"
     ]
    }
   ],
   "source": [
    "indicbert1_tokens_array = [char for sublist in indicbert1_tokens for char in sublist]\n",
    "indicbert1_tokens_map = frequency_decreasing(indicbert1_tokens, indicbert1_tokens_array)\n",
    "\n",
    "\n",
    "array_indicbert1 =[]\n",
    "\n",
    "# Storing the corrected unicode in array of list\n",
    "for i in range(len(indicbert1_tokens)):\n",
    "  array_indicbert1.append(correction(indicbert1_tokens[i]))\n",
    "\n",
    "indicbert1_array = [char for sublist in array_indicbert1 for char in sublist]\n",
    "indicbert1_bigram = makebigram(array_indicbert1)\n",
    "indicbert1_array_map = frequency_decreasing(array_indicbert1, indicbert1_array)\n",
    "indicbert1_bigram_map = frequency_decreasing(indicbert1_bigram, indicbert1_bigram)\n",
    "\n",
    "print(\"Top 20 frequent indicbert1 tokens\")\n",
    "count=0\n",
    "for element, freq in indicbert1_tokens_map.items():\n",
    "    print(f\"{element}: {freq}\")\n",
    "    count += 1\n",
    "    if count == 20:\n",
    "        break\n",
    "        \n",
    "print(\"Top 20 frequent indicbert1 characters\")\n",
    "count =0\n",
    "for element, freq in indicbert1_array_map.items():\n",
    "    print(f\"{element}: {freq}\")\n",
    "    count += 1\n",
    "    if count == 20:\n",
    "        break\n",
    "        \n",
    "print(\"Top 20 frequent indicbert1 bigram characters\")\n",
    "count =0\n",
    "for element, freq in indicbert1_bigram_map.items():\n",
    "    print(f\"{element}: {freq}\")\n",
    "    count += 1\n",
    "    if count == 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1dada487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 frequent indicbert1 syllable\n",
      "▁क: 960351\n",
      "▁ह: 444966\n",
      "य: 385246\n",
      "▁म: 283842\n",
      "त: 263047\n",
      "▁स: 242014\n",
      "▁पर: 241561\n",
      "▁न: 225179\n",
      "न: 190411\n",
      "क: 188131\n",
      "स: 146070\n",
      "ए: 134291\n",
      "▁कर: 125257\n",
      "▁द: 116469\n",
      "▁और: 115095\n",
      "र: 114722\n",
      "ग: 109763\n",
      "ष: 101499\n",
      "ल: 101277\n",
      "▁व: 99202\n",
      "Top 20 frequent indicbert1 syllable bigram\n",
      "त▁ह: 58174\n",
      "य▁क: 56561\n",
      "▁नह: 50048\n",
      "▁लए: 49617\n",
      "▁कय: 48148\n",
      "▁क▁ल: 44832\n",
      "न▁क: 39166\n",
      "▁रह▁ह: 36372\n",
      "▁ह▁क: 36354\n",
      "▁गय: 36160\n",
      "▁सथ: 35923\n",
      "य▁ह: 33275\n",
      "▁क▁स: 30494\n",
      "त▁क: 26922\n",
      "▁क▁पर: 26252\n",
      "▁दय: 22154\n",
      "▁उनक: 21924\n",
      "▁सकत: 21228\n",
      "▁हए: 19478\n",
      "▁इसक: 19095\n"
     ]
    }
   ],
   "source": [
    "indicbert1_syllable = []\n",
    "\n",
    "# Storing Syllables in array of list (syllable)\n",
    "for i in range(len(array_indicbert1)):\n",
    "  indicbert1_syllable.append(make_syllable(array_indicbert1[i]))\n",
    "\n",
    "indicbert1_syllable_array = [char for sublist in indicbert1_syllable for char in sublist]\n",
    "indicbert1_syllable_bigram = makebigram(indicbert1_syllable)\n",
    "indicbert1_syllable_array_map = frequency_decreasing(indicbert1_syllable, indicbert1_syllable_array)\n",
    "indicbert1_syllable_bigram_map = frequency_decreasing(indicbert1_syllable_bigram, indicbert1_syllable_bigram)\n",
    "\n",
    "print(\"Top 20 frequent indicbert1 syllable\")\n",
    "count =0\n",
    "for element, freq in indicbert1_syllable_array_map.items():\n",
    "    print(f\"{element}: {freq}\")\n",
    "    count += 1\n",
    "    if count == 20:\n",
    "        break\n",
    "        \n",
    "print(\"Top 20 frequent indicbert1 syllable bigram\")\n",
    "count =0\n",
    "for element, freq in indicbert1_syllable_bigram_map.items():\n",
    "    print(f\"{element}: {freq}\")\n",
    "    count += 1\n",
    "    if count == 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd7062db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#indic_bert(2k)\n",
    "\n",
    "indicbert2_tokens = []\n",
    "# Read the corpus file line by line and tokenize each line\n",
    "with open(corpus_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        line_tokens2 = tokenizer_indicbert2.tokenize(line.strip())\n",
    "        indicbert2_tokens.append(line_tokens2)\n",
    "\n",
    "indicbert2_tokens = [[word.replace('#', '') for word in sublist] for sublist in indicbert2_tokens]\n",
    "# print(len(indicbert2_tokens))\n",
    "\n",
    "# Now, indicbert2_tokens is a list of lists where each inner list contains tokens from a line of the corpus file\n",
    "\n",
    "# print(\"Tokenized Output:\", indicbert2_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6268f8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 frequent indicbert2 tokens\n",
      "▁क: 960351\n",
      "▁ह: 444966\n",
      "य: 385246\n",
      "▁म: 283842\n",
      "।: 267276\n",
      "▁: 263567\n",
      "त: 263047\n",
      "▁स: 242014\n",
      "▁पर: 241561\n",
      "▁न: 225179\n",
      "न: 190411\n",
      "क: 188131\n",
      ",: 180710\n",
      "स: 146070\n",
      "ए: 134291\n",
      "▁कर: 125257\n",
      "▁द: 116469\n",
      "▁और: 115095\n",
      "र: 114722\n",
      "ग: 109763\n",
      "Top 20 frequent indicbert2 characters\n",
      "अ: 12431407\n",
      "▁क्: 960351\n",
      "▁ह्: 444966\n",
      "य्: 385246\n",
      "▁म्: 283842\n",
      "त्: 263047\n",
      "▁स्: 242014\n",
      "▁पर्: 241561\n",
      "▁न्: 225179\n",
      "न्: 190411\n",
      "क्: 188131\n",
      "स्: 146070\n",
      "ए: 134291\n",
      "▁कर्: 125257\n",
      "▁द्: 116469\n",
      "▁और्: 115095\n",
      "र्: 114722\n",
      "ग्: 109763\n",
      "ष्: 101499\n",
      "ल्: 101277\n",
      "Top 20 frequent indicbert2 bigram characters\n",
      "▁क्अ: 960351\n",
      "अ▁क्: 930415\n",
      "▁ह्अ: 444966\n",
      "अ▁ह्: 430998\n",
      "य्अ: 385246\n",
      "अय्: 384935\n",
      "▁म्अ: 283842\n",
      "अ▁म्: 275702\n",
      "त्अ: 263047\n",
      "अत्: 262593\n",
      "▁स्अ: 242014\n",
      "▁पर्अ: 241561\n",
      "अ▁स्: 233744\n",
      "अ▁पर्: 230754\n",
      "▁न्अ: 225179\n",
      "अ▁न्: 217531\n",
      "न्अ: 190411\n",
      "अन्: 190393\n",
      "क्अ: 188131\n",
      "अक्: 187849\n"
     ]
    }
   ],
   "source": [
    "indicbert2_tokens_array = [char for sublist in indicbert2_tokens for char in sublist]\n",
    "indicbert2_tokens_map = frequency_decreasing(indicbert2_tokens, indicbert2_tokens_array)\n",
    "\n",
    "array_indicbert2 =[]\n",
    "\n",
    "# Storing the corrected unicode in array of list\n",
    "for i in range(len(indicbert2_tokens)):\n",
    "  array_indicbert2.append(correction(indicbert2_tokens[i]))\n",
    "\n",
    "indicbert2_array = [char for sublist in array_indicbert2 for char in sublist]\n",
    "indicbert2_bigram = makebigram(array_indicbert2)\n",
    "indicbert2_array_map = frequency_decreasing(array_indicbert2, indicbert2_array)\n",
    "indicbert2_bigram_map = frequency_decreasing(indicbert2_bigram, indicbert2_bigram)\n",
    "\n",
    "print(\"Top 20 frequent indicbert2 tokens\")\n",
    "count=0\n",
    "for element, freq in indicbert2_tokens_map.items():\n",
    "    print(f\"{element}: {freq}\")\n",
    "    count += 1\n",
    "    if count == 20:\n",
    "        break\n",
    "        \n",
    "print(\"Top 20 frequent indicbert2 characters\")\n",
    "count =0\n",
    "for element, freq in indicbert2_array_map.items():\n",
    "    print(f\"{element}: {freq}\")\n",
    "    count += 1\n",
    "    if count == 20:\n",
    "        break\n",
    "        \n",
    "print(\"Top 20 frequent indicbert2 bigram characters\")\n",
    "count =0\n",
    "for element, freq in indicbert2_bigram_map.items():\n",
    "    print(f\"{element}: {freq}\")\n",
    "    count += 1\n",
    "    if count == 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2cbeb9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 frequent indicbert2 syllable\n",
      "▁क: 960351\n",
      "▁ह: 444966\n",
      "य: 385246\n",
      "▁म: 283842\n",
      "त: 263047\n",
      "▁स: 242014\n",
      "▁पर: 241561\n",
      "▁न: 225179\n",
      "न: 190411\n",
      "क: 188131\n",
      "स: 146070\n",
      "ए: 134291\n",
      "▁कर: 125257\n",
      "▁द: 116469\n",
      "▁और: 115095\n",
      "र: 114722\n",
      "ग: 109763\n",
      "ष: 101499\n",
      "ल: 101277\n",
      "▁व: 99202\n",
      "Top 20 frequent indicbert2 syllable bigram\n",
      "त▁ह: 58174\n",
      "य▁क: 56561\n",
      "▁नह: 50048\n",
      "▁लए: 49617\n",
      "▁कय: 48148\n",
      "▁क▁ल: 44832\n",
      "न▁क: 39166\n",
      "▁रह▁ह: 36372\n",
      "▁ह▁क: 36354\n",
      "▁गय: 36160\n",
      "▁सथ: 35923\n",
      "य▁ह: 33275\n",
      "▁क▁स: 30494\n",
      "त▁क: 26922\n",
      "▁क▁पर: 26252\n",
      "▁दय: 22154\n",
      "▁उनक: 21924\n",
      "▁सकत: 21228\n",
      "▁हए: 19478\n",
      "▁इसक: 19095\n"
     ]
    }
   ],
   "source": [
    "indicbert2_syllable = []\n",
    "\n",
    "# Storing Syllables in array of list (syllable)\n",
    "for i in range(len(array_indicbert2)):\n",
    "  indicbert2_syllable.append(make_syllable(array_indicbert2[i]))\n",
    "\n",
    "indicbert2_syllable_array = [char for sublist in indicbert2_syllable for char in sublist]\n",
    "indicbert2_syllable_bigram = makebigram(indicbert2_syllable)\n",
    "indicbert2_syllable_array_map = frequency_decreasing(indicbert2_syllable, indicbert2_syllable_array)\n",
    "indicbert2_syllable_bigram_map = frequency_decreasing(indicbert2_syllable_bigram, indicbert2_syllable_bigram)\n",
    "\n",
    "print(\"Top 20 frequent indicbert2 syllable\")\n",
    "count =0\n",
    "for element, freq in indicbert2_syllable_array_map.items():\n",
    "    print(f\"{element}: {freq}\")\n",
    "    count += 1\n",
    "    if count == 20:\n",
    "        break\n",
    "        \n",
    "print(\"Top 20 frequent indicbert2 syllable bigram\")\n",
    "count =0\n",
    "for element, freq in indicbert2_syllable_bigram_map.items():\n",
    "    print(f\"{element}: {freq}\")\n",
    "    count += 1\n",
    "    if count == 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f31ed29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
